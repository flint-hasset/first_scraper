{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in c:\\users\\stefa\\anaconda3\\lib\\site-packages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "#!pip install BeautifulSoup4\n",
    "#!pip install urllib3\n",
    "#import urllib\n",
    "#import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from datetime import date "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass cookies to be used to authenticate logon at site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cookies = pd.read_csv(r'C:\\Users\\stefa\\Documents\\GitHub\\first_scraper-master\\cookies.csv',header=None).dropna()\n",
    "names = list(df_cookies[0])\n",
    "values = list(df_cookies[1])\n",
    "cookies = dict(zip(names, values))\n",
    "#cookies\n",
    "#df_cookies.set_index(df_cookies[0])#,drop=True, inplace=True)#.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add count to end URL, 0 for first 25, 25 for next 25, 50... \n",
    "season_avg_url = 'https://basketball.fantasysports.yahoo.com/nba/14392/players?status=ALL&pos=P&cut_type=33&stat1=S_AS_2018&myteam=0&sort=AR&sdir=1&count='\n",
    "day30_avg_url = 'https://basketball.fantasysports.yahoo.com/nba/14392/players?status=ALL&pos=P&cut_type=33&stat1=S_AL30&myteam=0&sort=AR&sdir=1&count='\n",
    "day14_avg_url = 'https://basketball.fantasysports.yahoo.com/nba/14392/players?status=ALL&pos=P&cut_type=33&stat1=S_AL14&myteam=0&sort=AR&sdir=1&count='\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull function\n",
    "- Pulls based on specified URL request\n",
    "- Cookies used to establish logon session from cookies.csv\n",
    "- Creates player array for 25 players at a time\n",
    "- Loop to create full datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_25(url,ranks):\n",
    "    r = requests.post(str(url+ranks), cookies=cookies)\n",
    "    soup = BeautifulSoup(r.text,'lxml')\n",
    "    table = soup.find('table',{'class':'Table Ta-start Fz-xs Table-mid Table-px-xs Table-interactive'})\n",
    "    mydivs = table.findAll('div', {\"class\": \"ysf-player-name Nowrap Grid-u Relative Lh-xs Ta-start\"})\n",
    "    player_list = []\n",
    "    for player in mydivs:\n",
    "        player_list.append(player.text)\n",
    "    df_player = pd.DataFrame(player_list)   \n",
    "    dummy_array = np.empty((27,23))\n",
    "    dummy_array[:] = np.nan\n",
    "    player_array = pd.DataFrame(dummy_array)\n",
    "    row_marker=0\n",
    "    for row in table.findAll('tr'):\n",
    "        column_marker = 0\n",
    "        for data in row.findAll('td'):\n",
    "            player_array.iloc[row_marker,column_marker] = data.get_text()\n",
    "            column_marker += 1\n",
    "        row_marker+=1\n",
    "    player_array['INJ'] = np.where(player_array[1].str.contains(\"INJ\"),\"INJ\",0)\n",
    "    player_array = player_array.dropna()\n",
    "    columns_we_like = [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,'INJ']\n",
    "    player_array = player_array[columns_we_like].reset_index(drop=True)\n",
    "    df_player = df_player.merge(player_array, left_index =True, right_index=True)\n",
    "    df_player.columns=['Player','Owner','GP','O_Rank','C_Rank','Owned','MP','FGs','FG_pct','FTs','FT_pct',\n",
    "                       '3pt','Pts','Rbs','Ast','Stl','Blk','TO','INJ']\n",
    "    return df_player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build top 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player = pull_25(day30_avg_url,str(0))\n",
    "for x in range(25,200,25):\n",
    "    df_temp = pull_25(day30_avg_url,str(x))\n",
    "    df_player = df_player.append(df_temp)\n",
    "    time.sleep(.25) \n",
    "df_player = df_player.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\stefa\\Documents\\GitHub\\first_scraper-master\\Data'\n",
    "today = str(date.today())\n",
    "\n",
    "df_player.to_csv(path+\"_\"+today+'_avg_30.csv',encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
